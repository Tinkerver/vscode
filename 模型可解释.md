医学领域：
尽管深度学习方法有着比较完备的数学统计原理，但对于给定任务的知识表征学习尚缺乏明确解释。深度学习的黑盒特性以及检查黑盒模型行为工具的缺乏影响了其在众多领域中的应用，比如医学领域以及金融领域、自动驾驶领域等。在这些领域中，所使用模型的可解释性和可靠性是影响最终用户信任的关键因素。由于深度学习模型不可解释，研究人员无法将模型中的神经元权重直接理解 / 解释为知识。此外，一些文章的研究结果表明，无论是激活的幅度或选择性，还是对网络决策的影响，都不足以决定一个神经元对给定任务的重要性 ，即，现有的深度学习模型中的主要参数和结构都不能直接解释模型。因此，在医学、金融、自动驾驶等领域中深度学习方法尚未实现广泛的推广应用。

可解释性在医学领域中是非常重要的。一个医疗诊断系统必须是透明的（transparent）、可理解的（understandable）、可解释的（explainable），以获得医生、监管者和病人的信任。

谷歌糖尿病视网膜病变的定级
[1]Development and Validation of a Deep Learning Algorithm for Detection of Diabetic Retinopathy in Retinal Fundus Photographs[J]. JAMA: the Journal of the American Medical Association, 2016, 316(22):2402-2410.
用12.8万张的眼底照片训练了模型，主要的测试数据是来自4997名病人的9963张眼底照片集EyePACS-1。敏感度为90.3%时特异度为98.1%，敏感度为97.5%时特异度为93.4%。

斯坦福两种皮肤病变的诊断
使用12.9万张含有两千多种病灶的临床皮肤照片训练出模型。测试了200-1000+张照片，获得的ROC曲线的AUC均大于0.94。

现在深度学习在医学方面的运用都是回顾性研究，没有涉及到前瞻性研究的领域。对于深度学习的运用来讲，使用拟合的方法来进行。比如在医学图像处理上，是对视觉领域的判别，识别人脸猫狗等可依据纯视觉领域，但对于医学，经济等实用，包含非常复杂的逻辑分析和对抽象概念的理解，和判断猫狗的简单特征的统计归纳不同。
识别的是对象的性质，而非对象的外观的相似性，大量的情况是外观形态相似但性质迥异，或外观差距很大但性质一致。

在自动驾驶中，需要可解释性确保车辆实时决策的可接受安全，提供关键交通场景中动作决策的可解释性和透明度以及遵守监管机构制定的所有交通规则



论文Interpretability of Deep Learning Models - A Survey of Results中，从多个维度描述可解释性：
1. 模型透明度
模型透明度由可模拟性，可分解性和算法透明度三个参数定义。
可模拟性指人是否能够重现所有的预测步骤，若可以则能够帮助理解模型参数变化。
可分解性是指模型参数是否有直观解释。
算法透明度是算法的工作原理是否可以解释。比如svm的分类是通过依据支持向量选择超平面实现，因此其具有算法透明。但深度网络多个非线性层后输出特征难以解释。

2. 模型功能性，文本描述



ai方面的应用，和学术方面有一定差异，实际中被用的大模型
金融，公司企业评级机构，债务级别等等。金融参考
可解释性-模型评判。研究时可用于参考评判

模型为什么做决定，可解释提升精度


各行业应用，多医疗领域归类，先看ai技术，分出深度模型查看可解释性的问题。